{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten,Dense, Dropout\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset:<br>\n",
    "train_dataset=original training set with no augmentation<br>\n",
    "back_dataset=training set with back-translation<br>\n",
    "aug_dataset=training set with eda-paraphrasing augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=pd.read_csv(r'..\\DAIC\\Preprocessed\\train_dataset.csv')\n",
    "back_dataset=pd.read_csv(r'..\\DAIC\\Preprocessed\\back_dataset.csv')\n",
    "aug_dataset=pd.read_csv(r'..\\DAIC\\Preprocessed\\aug_dataset.csv')\n",
    "test_dataset=pd.read_csv(r'..\\DAIC\\Preprocessed\\test_dataset.csv')\n",
    "val_dataset=pd.read_csv(r'..\\DAIC\\Preprocessed\\dev_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the no of datapoints and class balances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The samples in training dataset is: ',(len(train_dataset['response'])),'and the distribution is ',(train_dataset['PHQ8_Binary'].value_counts()))\n",
    "print(f'The samples in back dataset is: ',(len(back_dataset['response'])),'and the distribution is ',(back_dataset['PHQ8_Binary'].value_counts()))\n",
    "print(f'The samples in aug dataset is: ',(len(aug_dataset['response'])),'and the distribution is ',(aug_dataset['PHQ8_Binary'].value_counts()))\n",
    "print(f'The samples in validation dataset is: ',(len(val_dataset['response'])),'and the distribution is ',(val_dataset['PHQ8_Binary'].value_counts()))\n",
    "print(f'The samples in test dataset is: ',(len(test_dataset['response'])),'and the distribution is ',(test_dataset['PHQ8_Binary'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the training dataset. First of all we will do the classification without under/oversampling, word2vec and glove. After which we will use sampling balancing. We will do the test in the validation set and subsequently test set. We will do the same for back and aug dataset. At last we, will also try incorporating val into training dataset, as we have a separate test dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_dataset['response']\n",
    "X_back=back_dataset['response']\n",
    "X_aug=aug_dataset['response']\n",
    "X_val=val_dataset['response']\n",
    "X_test=test_dataset['response']\n",
    "\n",
    "y_train=train_dataset['PHQ8_Binary']\n",
    "y_back=back_dataset['PHQ8_Binary']\n",
    "y_aug=aug_dataset['PHQ8_Binary']\n",
    "y_val=val_dataset['PHQ8_Binary']\n",
    "y_test=test_dataset['PHQ8_Binary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Word2Vec</h1>\n",
    "<h3>Train_set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "word2vec_path='..\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin'\n",
    "word2vec=gensim.models.KeyedVectors.load_word2vec_format(word2vec_path,binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train_Set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=6300)\n",
    "tokenizer.fit_on_texts(train_dataset['response'])\n",
    "X_train_seq=tokenizer.texts_to_sequences(train_dataset['response'])\n",
    "X_val_seq=tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq=tokenizer.texts_to_sequences(X_test)\n",
    "length=len(max(X_train_seq,key=len))\n",
    "X_train_pad=pad_sequences(X_train_seq,maxlen=length)\n",
    "X_val_pad=pad_sequences(X_val_seq,maxlen=length)\n",
    "X_test_pad=pad_sequences(X_test_seq,maxlen=length)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=train_dataset['PHQ8_Binary'])\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix=np.zeros((len(word_index)+1,embedding_dim))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i]=word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    inputs=tf.keras.Input(length,embedding_dim)\n",
    "    x=Embedding(input_dim=len(word_index)+1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=length,\n",
    "                        trainable=False)(inputs)\n",
    "    x=Conv1D(filters=8,kernel_size=5,activation='relu',)(x)\n",
    "    x=MaxPooling1D(pool_size=2)(x)\n",
    "    x=Conv1D(filters=4,kernel_size=3,activation='relu')(x)\n",
    "    x=Flatten()(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    outputs=Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model=models.Model(inputs,outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_train_pad,y_train,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32,class_weight=class_weight_dict)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Undersampling\n",
    "rus=RandomUnderSampler(random_state=42)\n",
    "X_train_pad_un,y_train_un=rus.fit_resample(X_train_pad,y_train)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.003)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_train_pad_un,y_train_un,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random OverSampling\n",
    "smote=SMOTE(random_state=42)\n",
    "X_train_pad_smote,y_train_smote=smote.fit_resample(X_train_pad,y_train)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.003)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_train_pad_smote,y_train_smote,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Back_set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=6300)\n",
    "tokenizer.fit_on_texts(back_dataset['response'])\n",
    "X_back_seq=tokenizer.texts_to_sequences(back_dataset['response'])\n",
    "X_val_seq=tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq=tokenizer.texts_to_sequences(X_test)\n",
    "length=len(max(X_back_seq,key=len))\n",
    "X_back_pad=pad_sequences(X_back_seq,maxlen=length)\n",
    "X_val_pad=pad_sequences(X_val_seq,maxlen=length)\n",
    "X_test_pad=pad_sequences(X_test_seq,maxlen=length)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=back_dataset['PHQ8_Binary'])\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix=np.zeros((len(word_index)+1,embedding_dim))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i]=word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    inputs=tf.keras.Input(length,embedding_dim)\n",
    "    x=Embedding(input_dim=len(word_index)+1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=length,\n",
    "                        trainable=False)(inputs)\n",
    "    x=Conv1D(filters=8,kernel_size=5,activation='relu',)(x)\n",
    "    x=MaxPooling1D(pool_size=2)(x)\n",
    "    x=Conv1D(filters=4,kernel_size=3,activation='relu')(x)\n",
    "    x=Flatten()(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    outputs=Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model=models.Model(inputs,outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_back_pad,y_back,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32,class_weight=class_weight_dict)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Undersampling\n",
    "rus=RandomUnderSampler(random_state=42)\n",
    "X_back_pad_un,y_back_un=rus.fit_resample(X_back_pad,y_back)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_back_pad_un,y_back_un,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random OverSampling\n",
    "smote=SMOTE(random_state=42)\n",
    "X_back_pad_smote,y_back_smote=smote.fit_resample(X_back_pad,y_back)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.007)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_back_pad_smote,y_back_smote,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Aug_set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=6300)\n",
    "tokenizer.fit_on_texts(aug_dataset['response'])\n",
    "X_aug_seq=tokenizer.texts_to_sequences(aug_dataset['response'])\n",
    "X_val_seq=tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq=tokenizer.texts_to_sequences(X_test)\n",
    "length=len(max(X_aug_seq,key=len))\n",
    "X_aug_pad=pad_sequences(X_aug_seq,maxlen=length)\n",
    "X_val_pad=pad_sequences(X_val_seq,maxlen=length)\n",
    "X_test_pad=pad_sequences(X_test_seq,maxlen=length)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=aug_dataset['PHQ8_Binary'])\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix=np.zeros((len(word_index)+1,embedding_dim))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i]=word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    inputs=tf.keras.Input(length,embedding_dim)\n",
    "    x=Embedding(input_dim=len(word_index)+1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=length,\n",
    "                        trainable=False)(inputs)\n",
    "    x=Conv1D(filters=8,kernel_size=5,activation='relu',)(x)\n",
    "    x=MaxPooling1D(pool_size=2)(x)\n",
    "    x=Conv1D(filters=4,kernel_size=3,activation='relu')(x)\n",
    "    x=Flatten()(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    outputs=Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model=models.Model(inputs,outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_aug_pad,y_aug,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32,class_weight=class_weight_dict)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Undersampling\n",
    "rus=RandomUnderSampler(random_state=5)\n",
    "X_aug_pad_un,y_aug_un=rus.fit_resample(X_aug_pad,y_aug)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_aug_pad_un,y_aug_un,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random OverSampling\n",
    "smote=SMOTE(random_state=9)\n",
    "X_aug_pad_smote,y_aug_smote=smote.fit_resample(X_aug_pad,y_aug)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.007)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_aug_pad_smote,y_aug_smote,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GlOvE</h1>\n",
    "<h2>Creating Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index={}\n",
    "glove_path='../glove.6B.100d.txt'\n",
    "with open(glove_path,'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        coefs=np.asarray(values[1:],dtype='float32')\n",
    "        embedding_index[word]=coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train_Set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=6300)\n",
    "tokenizer.fit_on_texts(train_dataset['response'])\n",
    "X_train_seq=tokenizer.texts_to_sequences(train_dataset['response'])\n",
    "X_val_seq=tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq=tokenizer.texts_to_sequences(X_test)\n",
    "length=len(max(X_train_seq,key=len))\n",
    "X_train_pad=pad_sequences(X_train_seq,maxlen=length)\n",
    "X_val_pad=pad_sequences(X_val_seq,maxlen=length)\n",
    "X_test_pad=pad_sequences(X_test_seq,maxlen=length)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=train_dataset['PHQ8_Binary'])\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=100\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix=np.zeros((len(word_index)+1,embedding_dim))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector=embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    inputs=tf.keras.Input(length,embedding_dim)\n",
    "    x=Embedding(input_dim=len(word_index)+1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=length,\n",
    "                        trainable=False)(inputs)\n",
    "    x=Conv1D(filters=8,kernel_size=5,activation='relu',)(x)\n",
    "    x=MaxPooling1D(pool_size=2)(x)\n",
    "    x=Conv1D(filters=4,kernel_size=3,activation='relu')(x)\n",
    "    x=Flatten()(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    outputs=Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model=models.Model(inputs,outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.007)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_train_pad,y_train,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32,class_weight=class_weight_dict)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Undersampling\n",
    "rus=RandomUnderSampler(random_state=42)\n",
    "X_train_pad_un,y_train_un=rus.fit_resample(X_train_pad,y_train)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.003)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_train_pad_un,y_train_un,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random OverSampling\n",
    "smote=SMOTE(random_state=42)\n",
    "X_train_pad_smote,y_train_smote=smote.fit_resample(X_train_pad,y_train)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.003)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_train_pad_smote,y_train_smote,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Back_set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=6300)\n",
    "tokenizer.fit_on_texts(back_dataset['response'])\n",
    "X_back_seq=tokenizer.texts_to_sequences(back_dataset['response'])\n",
    "X_val_seq=tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq=tokenizer.texts_to_sequences(X_test)\n",
    "length=len(max(X_back_seq,key=len))\n",
    "X_back_pad=pad_sequences(X_back_seq,maxlen=length)\n",
    "X_val_pad=pad_sequences(X_val_seq,maxlen=length)\n",
    "X_test_pad=pad_sequences(X_test_seq,maxlen=length)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=back_dataset['PHQ8_Binary'])\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=100\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix=np.zeros((len(word_index)+1,embedding_dim))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector=embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    inputs=tf.keras.Input(length,embedding_dim)\n",
    "    x=Embedding(input_dim=len(word_index)+1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=length,\n",
    "                        trainable=False)(inputs)\n",
    "    x=Conv1D(filters=8,kernel_size=5,activation='relu',)(x)\n",
    "    x=MaxPooling1D(pool_size=2)(x)\n",
    "    x=Conv1D(filters=4,kernel_size=3,activation='relu')(x)\n",
    "    x=Flatten()(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    outputs=Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model=models.Model(inputs,outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_back_pad,y_back,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32,class_weight=class_weight_dict)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Undersampling\n",
    "rus=RandomUnderSampler(random_state=42)\n",
    "X_back_pad_un,y_back_un=rus.fit_resample(X_back_pad,y_back)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_back_pad_un,y_back_un,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random OverSampling\n",
    "smote=SMOTE(random_state=42)\n",
    "X_back_pad_smote,y_back_smote=smote.fit_resample(X_back_pad,y_back)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.007)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_back_pad_smote,y_back_smote,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Aug_set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=6300)\n",
    "tokenizer.fit_on_texts(aug_dataset['response'])\n",
    "X_aug_seq=tokenizer.texts_to_sequences(aug_dataset['response'])\n",
    "X_val_seq=tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq=tokenizer.texts_to_sequences(X_test)\n",
    "length=len(max(X_aug_seq,key=len))\n",
    "X_aug_pad=pad_sequences(X_aug_seq,maxlen=length)\n",
    "X_val_pad=pad_sequences(X_val_seq,maxlen=length)\n",
    "X_test_pad=pad_sequences(X_test_seq,maxlen=length)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=aug_dataset['PHQ8_Binary'])\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=100\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix=np.zeros((len(word_index)+1,embedding_dim))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector=embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    inputs=tf.keras.Input(length,embedding_dim)\n",
    "    x=Embedding(input_dim=len(word_index)+1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=length,\n",
    "                        trainable=False)(inputs)\n",
    "    x=Conv1D(filters=8,kernel_size=5,activation='relu',)(x)\n",
    "    x=MaxPooling1D(pool_size=2)(x)\n",
    "    x=Conv1D(filters=4,kernel_size=3,activation='relu')(x)\n",
    "    x=Flatten()(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    outputs=Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model=models.Model(inputs,outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_aug_pad,y_aug,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32,class_weight=class_weight_dict)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Undersampling\n",
    "rus=RandomUnderSampler(random_state=5)\n",
    "X_aug_pad_un,y_aug_un=rus.fit_resample(X_aug_pad,y_aug)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_aug_pad_un,y_aug_un,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random OverSampling\n",
    "smote=SMOTE(random_state=9)\n",
    "X_aug_pad_smote,y_aug_smote=smote.fit_resample(X_aug_pad,y_aug)\n",
    "cnn=train_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.007)\n",
    "cnn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_aug_pad_smote,y_aug_smote,epochs=15,validation_data=(X_val_pad,y_val),batch_size=32)\n",
    "\n",
    "y_test_pred=cnn.predict(X_test_pad)\n",
    "y_test_pred=(y_test_pred>=0.5).astype(int).reshape(-1)\n",
    "\n",
    "print('Test Set Performance:')\n",
    "print(classification_report(y_test,y_test_pred,target_names=['Controlled','Depression'],zero_division=0.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
