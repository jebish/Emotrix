{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pWcFO3xU3ZrD",
        "outputId": "269138e0-9e93-498f-e03e-84cffc147edf"
      },
      "outputs": [],
      "source": [
        "#Run this to install dependencies\n",
        "!pip install fastapi uvicorn nest-asyncio pyngrok\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF6vSLFs3cie",
        "outputId": "318c9822-70ce-436d-81c9-08cf886bac93"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from transformers import TextStreamer\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Unzip the lora_model file saved during text classification fine tuning\n",
        "!unzip '/content/lora_model.zip' -d 'lora_directory'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jooIf9oy4WCM",
        "outputId": "e000cffd-f6af-41dc-d1fa-634e114655a7"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 6000\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_directory\", #Insert your LoRA unzipped directory\n",
        "        #Unsloth will automatically download the merged model from huggingface and apply LoRA adapters\n",
        "        #Alternatively you may also give huggingface LoRA adapter link directly\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True\n",
        "    )\n",
        "\n",
        "FastLanguageModel.for_inference(model) #Ready the model for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mmbi90Xt7Bb4"
      },
      "outputs": [],
      "source": [
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "\n",
        "#This function generates text i.e. asks questions\n",
        "def generate_response(prompt):\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      prompt,\n",
        "      tokenize = True,\n",
        "      add_generation_prompt = True, # Must add for generation\n",
        "      return_tensors = \"pt\",\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  model_out = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 256,\n",
        "                    use_cache = True, temperature = 0.7, min_p = 0.1)\n",
        "  decoded_output = tokenizer.decode(model_out[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "  return decoded_output\n",
        "\n",
        "#This function classifies the conversation i.e. analyzes the emotional state of user\n",
        "def classify_conversation(prompt):\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": 'Below is an instruction with Input that describes a task, write a response that appropriately completes the task'},\n",
        "  ]\n",
        "  datum='''Instruction: Analyze the conversation in the input and categorize if the user is Depressed or not with reasoning. Reason it and formulate the answer. Use second person and empathic tone.\n",
        "  Input:'''\n",
        "  for row in prompt[1:]:\n",
        "    datum=datum+\" \"+row['role']+': '+row['content']\n",
        "  messages.append({'role':'user','content':datum})\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = True,\n",
        "      add_generation_prompt = True, # Must add for generation\n",
        "      return_tensors = \"pt\",\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  model_out = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 512,\n",
        "                    use_cache = True, temperature = 0.1, min_p = 0.1)\n",
        "  decoded_output = tokenizer.decode(model_out[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "  return decoded_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RIR6he0Y_qh2"
      },
      "outputs": [],
      "source": [
        "#Import the fastapi framework to create an API\n",
        "app=FastAPI()\n",
        "\n",
        "#Use Pydantic for data validation to ensure that the data matches the required input to model\n",
        "class PromptRequest(BaseModel):\n",
        "  task:str\n",
        "  prompt: str\n",
        "\n",
        "#For running asynchronous code in Notebook i.e. Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "#Set the authentication token for your ngrok\n",
        "#ngrok allows to establish a secure tunnel to localhost for external access\n",
        "ngrok.set_auth_token(\"\")\n",
        "\n",
        "#Initialize conversation history file\n",
        "conversation_history=[{\"role\": \"system\", \"content\": \"You are an assistant. Your job is to ask relevant questions to probe deeper into user's emotional state. Follow DAIC-woz/PHQ8 schema and ask all questions but DONT tell user that you are using DAIC woz schema. Use your fine tuning. Ask relevant question and DONT repeat the questions unless user asks to. Ask one question at a time. Use some emotional context to enrich the experience. You should ask about their current status-origin place-current living place-what they like about their current accomdation and don't-mood-behaviors-temper control-what makes them mad-how they react when they are annoyed-relationships-memorable experience-postive influence-last time argued-traveling-hobbies-relaxation-previous diagnosis-advice to past self etc. Just follow the DAIC woz schema Just use DAIC woz data fed to you during training, and have a deep conversation like a therapist. Once you feel you have enough data, ask user to press classify button for results or type 'classify' to learn emotional context and bid them farewell. Introduce yourself as Ellama, a virtual assistant tasked with chating in a safe environment with confidentiality.\"}]\n",
        "\n",
        "#POST Access point definition\n",
        "@app.post(\"/predict/\")\n",
        "async def predict(prompt_request:PromptRequest):\n",
        "  \n",
        "  #Append the user message to the conversation history\n",
        "  if prompt_request.task=='classify':\n",
        "    result= classify_conversation(conversation_history)\n",
        "    return {'response':result}\n",
        "  elif prompt_request.task=='reset':\n",
        "    conversation_history.clear()\n",
        "    conversation_history.append({\"role\": \"system\", \"content\": \"You are an assistant. Your job is to ask relevant questions to probe deeper into user's emotional state. Follow DAIC-woz/PHQ8 schema and ask all questions but DONT tell user that you are using DAIC woz schema. Use your fine tuning. Ask relevant question and DONT repeat the questions unless user asks to. Ask one question at a time. Use some emotional context to enrich the experience. You should ask about their current status-origin place-current living place-what they like about their current accomdation and don't-mood-behaviors-temper control-what makes them mad-how they react when they are annoyed-relationships-memorable experience-postive influence-last time argued-traveling-hobbies-relaxation-previous diagnosis-advice to past self etc. Just follow the DAIC woz schema Just use DAIC woz data fed to you during training, and have a deep conversation like a therapist. Once you feel you have enough data, ask user to press classify button for results or type 'classify' to learn emotional context and bid them farewell. Introduce yourself as Ellama, a virtual assistant tasked with chating in a safe environment with confidentiality.\"})\n",
        "    return{\"response\":\"Cleared\"}\n",
        "  else:\n",
        "    #Append the user message to the conversation history\n",
        "    conversation_history.append({'role':'user','content':prompt_request.prompt})\n",
        "    response=generate_response(conversation_history)\n",
        "    #Append the model response to the conversation history\n",
        "    conversation_history.append({'role':'assistant','content':response})\n",
        "    return {\"response\":response}\n",
        "\n",
        "#Create a public url using ngrok\n",
        "public_url=ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "#Start the fast api on port 8000\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
